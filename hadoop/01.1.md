#### 下载

```bash
wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.0.0/hadoop-3.0.0.tar.gz  
```

####　配置文件修改

```bash
cd hadoop-3.0.0/etc/hadoop

#core-site.xml
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
</property>

#hdfs-site.xml
mkdir -p /data/namenode /data/datanode
<property>
    <name>dfs.replication</name>
    <value>2</value>
</property>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>/data/hd3/namenode</value>
</property>
<property>
    <name>dfs.datanode.data.dir</name>
    <value>/data/hd3/datanode</value>
</property>

#yarn-site.xml
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.nodemanager.env-whitelist</name>
    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
<property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>49152</value>
</property>
<property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>49152</value>
</property>

#mapred-site.xml
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

#注意在3.0.0版本里面slave文件改成了workers
slave1
slave2

#hadoop-env.sh
export JAVA_HOME=/usr/java/jdk1.8.0_151
export HDFS_NAMENODE_USER="root"
export HDFS_DATANODE_USER="root"
export HDFS_SECONDARYNAMENODE_USER="root"
export YARN_RESOURCEMANAGER_USER="root"
export YARN_NODEMANAGER_USER="root"

#配置环境变量
sudo vim /etc/profile
export HADOOP_PREFIX=/opt/software/hadoop-3.0.0
export HADOOP_HOME=/opt/software/hadoop-3.0.0
export HADOOP_HDFS_HOME=/opt/software/hadoop-3.0.0
export HADOOP_CONF_DIR=/opt/software/hadoop-3.0.0/etc/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
source /etc/profile
```

#### 启动hadoop

```bash
hdfs namenode -format
```

#### 启动HDFS

```bash
start-dfs-sh
```

#### 启动YARN

```bash
start-yarn.sh
```